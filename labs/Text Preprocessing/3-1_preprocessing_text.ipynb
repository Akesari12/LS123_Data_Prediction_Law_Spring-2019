{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Preprocessing Text - Lab 3-1\n",
    "\n",
    "---\n",
    "\n",
    "This lab will provide an introduction to manipulating strings and chunking sentences.\n",
    "\n",
    "*Estimated Time: 30-40 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "### Topics Covered\n",
    "- How to tokenize text\n",
    "- How to stem text\n",
    "- How to chunk text\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[The Data](#section data)<br>\n",
    "\n",
    "1 - [Tokenization](#section 1)<br>\n",
    "\n",
    "2 - [Stemming](#section 2)<br>\n",
    "\n",
    "3 - [Chunking](#section 3)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anike\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\anike\\anaconda3\\lib\\site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Data <a id='data'></a>\n",
    "\n",
    "\n",
    "In this notebook, you'll be working with the text of each country’s statement from the General Debate in annual sessions of the United Nations General Assembly. This dataset is separated by country, session and year and tagged for each, and has over forty years of data from different countries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cells and take a look at a sample of the data that we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/un-general-debates.zip\", compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1989</td>\n",
       "      <td>MDV</td>\n",
       "      <td>﻿It is indeed a pleasure for me and the member...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>1989</td>\n",
       "      <td>FIN</td>\n",
       "      <td>﻿\\nMay I begin by congratulating you. Sir, on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1989</td>\n",
       "      <td>NER</td>\n",
       "      <td>﻿\\nMr. President, it is a particular pleasure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>1989</td>\n",
       "      <td>URY</td>\n",
       "      <td>﻿\\nDuring the debate at the fortieth session o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>1989</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>﻿I should like at the outset to express my del...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  year country                                               text\n",
       "0       44  1989     MDV  ﻿It is indeed a pleasure for me and the member...\n",
       "1       44  1989     FIN  ﻿\\nMay I begin by congratulating you. Sir, on ...\n",
       "2       44  1989     NER  ﻿\\nMr. President, it is a particular pleasure ...\n",
       "3       44  1989     URY  ﻿\\nDuring the debate at the fortieth session o...\n",
       "4       44  1989     ZWE  ﻿I should like at the outset to express my del..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization  <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is defined as <b>the process of segmenting running text into words and sentences</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electronic text is a linear sequence of symbols. Before any processing is to be done, text needs to be segmented into linguistic units, and this process is called tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually look at grammar and meaning at the level of words, related to each other within sentences, within each document. So if we're starting with raw text, we first need to split the text into sentences, and those sentences into words -- which we call \"tokens\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might imagine that the easiest way to identify sentences is to split the document at every period '.', and to split the sentences using white space to get the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿It is indeed a pleasure for me and the members of my delegation to extend to Ambassador Garba our sincere congratulations on his election to the presidency of the forty-fourth session of the General Assembly\n",
      "\n",
      " His election to this high office is a well-deserved tribute to his personal qualities and experience\n",
      "\n",
      " I am fully confident that under his able and wise leadership the Assembly will further consolidate the gains achieved during the past year\n",
      "\n",
      "\n",
      "My delegation associates itself with previous speakers in expressing its appreciation of the dedicated efforts of his predecessor, His Excellency Mr\n",
      "\n",
      " Dante Caputo, for the exemplary manner in which he discharged his duties as President of the forty-third session of the General Assembly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the split function to create tokens\n",
    "paragraph = data['text'][0]\n",
    "sentences = paragraph.split(\".\")\n",
    "for s in sentences[:5]:\n",
    "    print(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to split sentences further into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'kind', 'of', 'patterns', 'do', 'you', 'see', 'in', 'this', 'graph?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"What kind of patterns do you see in this graph?\"\n",
    "tokens = sentence.split(\" \")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stop here as NLTK provides handy tools for us to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a platform for building Python programs to work with human language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# run the below commented command if error\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\ufeffI should like at the outset to express my delegation's satisfaction and pleasure at your election, Sir, to the presidency of the General Assembly at its forty-fourth session.\",\n",
       " 'The unanimity of that decision reflects not only your own distinguished record as Foreign Minister and Permanent Representative of your country to the United Nations but also the prestige of your country, Nigeria, of which all of us in Africa are proud.',\n",
       " 'The outgoing President of the General Assembly, Mr. Dante Caputo of Argentina, shouldered the responsibility of his office with distinction in a momentous and difficult year.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sentence tokens\n",
    "speech = data['text'][4]\n",
    "sents = nltk.sent_tokenize(speech)\n",
    "sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
    "nltk.word_tokenize(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk recognized that \"o'clock\" is one word and separated \"didn't\" into \"did\" and \"n't\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated metrics, it's easier to use NLTK's classes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 401),\n",
       " ('of', 213),\n",
       " (',', 180),\n",
       " ('to', 177),\n",
       " ('.', 175),\n",
       " ('and', 139),\n",
       " ('in', 106),\n",
       " ('that', 88),\n",
       " ('a', 70),\n",
       " ('is', 63)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 10 most common tokens\n",
    "tokens = nltk.word_tokenize(speech)\n",
    "fd = nltk.collocations.FreqDist(tokens)\n",
    "fd.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so interesting as the most common words seem to be words that have no particular meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common step in text analysis is to remove noise. *However*, what you deem \"noise\" is not only very important but also dependent on the project at hand. For the purposes of today, we will discuss two common categories of strings often considered \"noise\". \n",
    "\n",
    "- Punctuation: While important for sentence analysis, punctuation will get in the way of word frequency and n-gram analyses. They will also affect any clustering on topic modeling.\n",
    "\n",
    "- Stopwords: Stopwords are the most frequent words in any given language. Words like \"the\", \"a\", \"that\", etc. are considered not semantically important, and would also skew any frequency or n-gram analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b> Write a function below that takes a string as an argument and returns a list of words without punctuation or stopwords.\n",
    "\n",
    "`punctuation` is a list of punctuation strings, and we have created the list `stop_words` for you.\n",
    "\n",
    "Hint: first you'll want to remove punctuation, then tokenize, then remove stop words. Make sure you account for upper and lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_punc_stop(text):\n",
    "    \n",
    "    from string import punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    punctuation = set(punctuation)\n",
    "    \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    \n",
    "    words = nltk.word_tokenize(punc_free)\n",
    "    \n",
    "    noise_free = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return noise_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rerun our frequency analysis without the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffI',\n",
       " 'like',\n",
       " 'outset',\n",
       " 'express',\n",
       " 'delegations',\n",
       " 'satisfaction',\n",
       " 'pleasure',\n",
       " 'election',\n",
       " 'Sir',\n",
       " 'presidency',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'fortyfourth',\n",
       " 'session',\n",
       " 'The',\n",
       " 'unanimity',\n",
       " 'decision',\n",
       " 'reflects',\n",
       " 'distinguished',\n",
       " 'record',\n",
       " 'Foreign',\n",
       " 'Minister',\n",
       " 'Permanent',\n",
       " 'Representative',\n",
       " 'country',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'also',\n",
       " 'prestige',\n",
       " 'country',\n",
       " 'Nigeria',\n",
       " 'us',\n",
       " 'Africa',\n",
       " 'proud',\n",
       " 'The',\n",
       " 'outgoing',\n",
       " 'President',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'Mr',\n",
       " 'Dante',\n",
       " 'Caputo',\n",
       " 'Argentina',\n",
       " 'shouldered',\n",
       " 'responsibility',\n",
       " 'office',\n",
       " 'distinction',\n",
       " 'momentous',\n",
       " 'difficult',\n",
       " 'year',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'acknowledge',\n",
       " 'debt',\n",
       " 'Our',\n",
       " 'SecretaryGeneral',\n",
       " 'Mr',\n",
       " 'Javier',\n",
       " 'Perez',\n",
       " 'de',\n",
       " 'Cuellar',\n",
       " 'head',\n",
       " 'Organization',\n",
       " 'roost',\n",
       " 'troubled',\n",
       " 'also',\n",
       " 'productive',\n",
       " 'successful',\n",
       " 'years',\n",
       " 'The',\n",
       " 'turnaround',\n",
       " 'fortunes',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'watch',\n",
       " 'owes',\n",
       " 'much',\n",
       " 'skill',\n",
       " 'helmsman',\n",
       " 'want',\n",
       " 'reassure',\n",
       " 'continued',\n",
       " 'confidence',\n",
       " 'The',\n",
       " 'current',\n",
       " 'session',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'must',\n",
       " 'seriously',\n",
       " 'address',\n",
       " 'problems',\n",
       " 'affecting',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'In',\n",
       " 'vital',\n",
       " 'field',\n",
       " 'international',\n",
       " 'economic',\n",
       " 'relations',\n",
       " 'fresh',\n",
       " 'breeze',\n",
       " 'led',\n",
       " 'excitement',\n",
       " 'international',\n",
       " 'political',\n",
       " 'relations',\n",
       " 'even',\n",
       " 'made',\n",
       " 'ripple',\n",
       " 'The',\n",
       " 'direction',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'still',\n",
       " 'determined',\n",
       " 'small',\n",
       " 'circle',\n",
       " 'rich',\n",
       " 'powerful',\n",
       " 'nations',\n",
       " 'shortterm',\n",
       " 'expediency',\n",
       " 'requirements',\n",
       " 'individual',\n",
       " 'national',\n",
       " 'economies',\n",
       " 'often',\n",
       " 'taken',\n",
       " 'precedence',\n",
       " 'steps',\n",
       " 'required',\n",
       " 'healthier',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'States',\n",
       " 'would',\n",
       " 'benefit',\n",
       " 'long',\n",
       " 'run',\n",
       " 'For',\n",
       " 'smaller',\n",
       " 'countries',\n",
       " 'result',\n",
       " 'collapse',\n",
       " 'commodity',\n",
       " 'prices',\n",
       " 'causing',\n",
       " 'severe',\n",
       " 'prejudicial',\n",
       " 'terms',\n",
       " 'trade',\n",
       " 'several',\n",
       " 'times',\n",
       " 'volume',\n",
       " 'quality',\n",
       " 'produce',\n",
       " 'required',\n",
       " 'purchase',\n",
       " 'amount',\n",
       " 'manufactured',\n",
       " 'goods',\n",
       " 'North',\n",
       " 'required',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'necessary',\n",
       " 'borrowing',\n",
       " 'part',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'coupled',\n",
       " 'high',\n",
       " 'interest',\n",
       " 'rates',\n",
       " 'resulted',\n",
       " 'debilitating',\n",
       " 'debtservice',\n",
       " 'burden',\n",
       " 'countries',\n",
       " 'culminated',\n",
       " 'net',\n",
       " 'reverse',\n",
       " 'transfer',\n",
       " 'resources',\n",
       " 'South',\n",
       " 'North',\n",
       " 'order',\n",
       " 'nearly',\n",
       " '50',\n",
       " 'billion',\n",
       " 'annually',\n",
       " 'according',\n",
       " 'calculations',\n",
       " 'international',\n",
       " 'financial',\n",
       " 'institutions',\n",
       " 'Fifty',\n",
       " 'billion',\n",
       " 'dollars',\n",
       " 'annually',\n",
       " 'big',\n",
       " 'amount',\n",
       " 'owe',\n",
       " 'ate',\n",
       " 'informed',\n",
       " 'money',\n",
       " 'still',\n",
       " 'continuing',\n",
       " 'flow',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'developed',\n",
       " 'countries',\n",
       " 'How',\n",
       " 'conscience',\n",
       " 'talk',\n",
       " 'kinder',\n",
       " 'gentler',\n",
       " 'times',\n",
       " 'continue',\n",
       " 'countenance',\n",
       " 'continual',\n",
       " 'transfer',\n",
       " 'phenomenal',\n",
       " 'amount',\n",
       " 'resources',\n",
       " 'poor',\n",
       " 'hungry',\n",
       " 'rich',\n",
       " 'affluent',\n",
       " 'We',\n",
       " 'welcome',\n",
       " 'positive',\n",
       " 'developments',\n",
       " 'international',\n",
       " 'political',\n",
       " 'stage',\n",
       " 'believe',\n",
       " 'peoples',\n",
       " 'enjoy',\n",
       " 'political',\n",
       " 'civil',\n",
       " 'rights',\n",
       " 'Conscience',\n",
       " 'however',\n",
       " 'enjoins',\n",
       " 'us',\n",
       " 'submit',\n",
       " 'rights',\n",
       " 'enough',\n",
       " 'People',\n",
       " 'economic',\n",
       " 'social',\n",
       " 'rights',\n",
       " 'well',\n",
       " 'right',\n",
       " 'good',\n",
       " 'health',\n",
       " 'right',\n",
       " 'food',\n",
       " 'shelter',\n",
       " 'tight',\n",
       " 'decent',\n",
       " 'standard',\n",
       " 'living',\n",
       " 'It',\n",
       " 'lot',\n",
       " 'one',\n",
       " 'suffer',\n",
       " 'degradations',\n",
       " 'illiteracy',\n",
       " 'poverty',\n",
       " 'Hunan',\n",
       " 'rights',\n",
       " 'package',\n",
       " 'elements',\n",
       " 'mutually',\n",
       " 'reinforcing',\n",
       " 'denial',\n",
       " 'weakens',\n",
       " 'entire',\n",
       " 'package',\n",
       " 'A',\n",
       " 'poor',\n",
       " 'man',\n",
       " 'sick',\n",
       " 'man',\n",
       " 'illiterate',\n",
       " 'man',\n",
       " 'manacled',\n",
       " 'man',\n",
       " 'denied',\n",
       " 'individual',\n",
       " 'freedom',\n",
       " 'right',\n",
       " 'free',\n",
       " 'speech',\n",
       " 'It',\n",
       " 'package',\n",
       " 'important',\n",
       " 'goes',\n",
       " 'beyond',\n",
       " 'selective',\n",
       " 'times',\n",
       " 'cynical',\n",
       " 'championing',\n",
       " 'right',\n",
       " 'core',\n",
       " 'purpose',\n",
       " 'existence·',\n",
       " 'tight',\n",
       " 'decent',\n",
       " 'meaningful',\n",
       " 'life',\n",
       " 'Any',\n",
       " 'attempt',\n",
       " 'separate',\n",
       " 'one',\n",
       " 'compartmentalize',\n",
       " 'rights',\n",
       " 'champion',\n",
       " 'one',\n",
       " 'right',\n",
       " 'expense',\n",
       " 'sow',\n",
       " 'confusion',\n",
       " 'set',\n",
       " 'stage',\n",
       " 'futile',\n",
       " 'recriminations',\n",
       " 'All',\n",
       " 'States',\n",
       " 'represented',\n",
       " 'Assembly',\n",
       " 'subscribe',\n",
       " 'view',\n",
       " 'attempt',\n",
       " 'pursue',\n",
       " 'objective',\n",
       " 'good',\n",
       " 'decent',\n",
       " 'existence',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'The',\n",
       " 'pursuit',\n",
       " 'healthier',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'growth',\n",
       " 'development',\n",
       " 'poorer',\n",
       " 'countries',\n",
       " 'fairer',\n",
       " 'terms',\n",
       " 'trade',\n",
       " 'North',\n",
       " 'South',\n",
       " 'fundamentally',\n",
       " 'struggle',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'tight',\n",
       " 'people',\n",
       " 'decent',\n",
       " 'existence',\n",
       " 'In',\n",
       " 'interdependent',\n",
       " 'world',\n",
       " 'struggle',\n",
       " 'must',\n",
       " 'involve',\n",
       " 'concerted',\n",
       " 'approach',\n",
       " 'including',\n",
       " 'greater',\n",
       " 'recourse',\n",
       " 'multilateral',\n",
       " 'mechanisms',\n",
       " 'part',\n",
       " 'States',\n",
       " 'Nations',\n",
       " 'must',\n",
       " 'act',\n",
       " 'together',\n",
       " 'solve',\n",
       " 'global',\n",
       " 'interrelated',\n",
       " 'problems',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'low',\n",
       " 'commodity',\n",
       " 'prices',\n",
       " 'high',\n",
       " 'interest',\n",
       " 'rates',\n",
       " 'acute',\n",
       " 'external',\n",
       " 'Indebtedness',\n",
       " 'The',\n",
       " 'politics',\n",
       " 'inclusion',\n",
       " 'appear',\n",
       " 'gaining',\n",
       " 'ground',\n",
       " 'international',\n",
       " 'political',\n",
       " 'relations',\n",
       " 'particularly',\n",
       " 'professions',\n",
       " 'big',\n",
       " 'Powers',\n",
       " 'readiness',\n",
       " 'give',\n",
       " 'larger',\n",
       " 'role',\n",
       " 'management',\n",
       " 'international',\n",
       " 'affairs',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'also',\n",
       " 'reflected',\n",
       " 'international',\n",
       " 'economic',\n",
       " 'relations',\n",
       " 'The',\n",
       " 'initiative',\n",
       " 'taken',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'urging',\n",
       " 'Assembly',\n",
       " 'hold',\n",
       " 'special',\n",
       " 'session',\n",
       " 'economic',\n",
       " 'cooperation',\n",
       " 'particular',\n",
       " 'revitalization',\n",
       " 'growth',\n",
       " 'development',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'April',\n",
       " '1990',\n",
       " 'timely',\n",
       " 'development',\n",
       " 'The',\n",
       " 'special',\n",
       " 'session',\n",
       " 'focus',\n",
       " 'need',\n",
       " 'renew',\n",
       " 'stalled',\n",
       " 'NorthSouth',\n",
       " 'dialogue',\n",
       " 'direction',\n",
       " 'world',\n",
       " 'economy',\n",
       " 'I',\n",
       " 'pleased',\n",
       " 'note',\n",
       " 'recent',\n",
       " 'summit',\n",
       " 'Conference',\n",
       " 'Countries',\n",
       " 'NonAligned',\n",
       " 'Movement',\n",
       " 'held',\n",
       " 'Belgrade',\n",
       " 'endorsed',\n",
       " 'suggestion',\n",
       " 'member',\n",
       " 'States',\n",
       " 'NonAligned',\n",
       " 'Movement',\n",
       " 'cooperating',\n",
       " 'exercise',\n",
       " 'In',\n",
       " 'era',\n",
       " 'economic',\n",
       " 'megablocs',\n",
       " 'process',\n",
       " 'formed',\n",
       " 'North',\n",
       " 'America',\n",
       " 'Europe',\n",
       " 'East',\n",
       " 'West',\n",
       " 'drawing',\n",
       " 'closer',\n",
       " 'many',\n",
       " 'spheres',\n",
       " 'essential',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'left',\n",
       " 'discussions',\n",
       " 'mechanisms',\n",
       " 'affect',\n",
       " 'future',\n",
       " 'course',\n",
       " 'international',\n",
       " 'economic',\n",
       " 'political',\n",
       " 'relations',\n",
       " 'We',\n",
       " 'also',\n",
       " 'note',\n",
       " 'preparations',\n",
       " 'way',\n",
       " 'elaboration',\n",
       " 'international',\n",
       " 'development',\n",
       " 'strategy',\n",
       " 'Fourth',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'Development',\n",
       " 'Decade',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'stress',\n",
       " 'complementarity',\n",
       " 'see',\n",
       " 'process',\n",
       " 'next',\n",
       " 'years',\n",
       " 'special',\n",
       " 'session',\n",
       " 'economic',\n",
       " 'cooperation',\n",
       " 'I',\n",
       " 'already',\n",
       " 'referred',\n",
       " 'Following',\n",
       " '1986',\n",
       " 'special',\n",
       " 'session',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'critical',\n",
       " 'economic',\n",
       " 'situation',\n",
       " 'Africa',\n",
       " 'Assembly',\n",
       " 'adopted',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'Programme',\n",
       " 'Action',\n",
       " 'African',\n",
       " 'Economic',\n",
       " 'Recovery',\n",
       " 'Development',\n",
       " '19861990',\n",
       " 'That',\n",
       " 'Programme',\n",
       " 'represented',\n",
       " 'compact',\n",
       " 'African',\n",
       " 'countries',\n",
       " 'international',\n",
       " 'community',\n",
       " 'especially',\n",
       " 'donor',\n",
       " 'countries',\n",
       " 'The',\n",
       " 'African',\n",
       " 'countries',\n",
       " 'take',\n",
       " 'steps',\n",
       " 'redress',\n",
       " 'economies',\n",
       " 'structural',\n",
       " 'adjustment',\n",
       " 'programmes',\n",
       " 'done',\n",
       " 'often',\n",
       " 'significant',\n",
       " 'social',\n",
       " 'political',\n",
       " 'cost',\n",
       " 'international',\n",
       " 'community',\n",
       " 'undertook',\n",
       " 'create',\n",
       " 'international',\n",
       " 'environment',\n",
       " 'conducive',\n",
       " 'African',\n",
       " 'recovery',\n",
       " 'facilitating',\n",
       " 'increase',\n",
       " 'commodity',\n",
       " 'prices',\n",
       " 'resource',\n",
       " '21ows',\n",
       " 'continent',\n",
       " 'The',\n",
       " 'midterm',\n",
       " 'review',\n",
       " 'implementation',\n",
       " 'Programme',\n",
       " 'annexed',\n",
       " 'resolution',\n",
       " '4327',\n",
       " '1988',\n",
       " 'clearly',\n",
       " 'states',\n",
       " 'international',\n",
       " 'community',\n",
       " 'kept',\n",
       " 'side',\n",
       " 'bargain',\n",
       " 'The',\n",
       " 'African',\n",
       " 'States',\n",
       " 'made',\n",
       " 'adjustments',\n",
       " 'required',\n",
       " 'Let',\n",
       " 'echo',\n",
       " 'words',\n",
       " 'Mr',\n",
       " 'Michael',\n",
       " 'Manley',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'Jamaica',\n",
       " 'sand',\n",
       " 'recently',\n",
       " 'We',\n",
       " 'done',\n",
       " 'everything',\n",
       " 'asked',\n",
       " 'us',\n",
       " 'But',\n",
       " 'side',\n",
       " 'done',\n",
       " 'Commodity',\n",
       " 'prices',\n",
       " 'continued',\n",
       " 'fall',\n",
       " 'resource',\n",
       " 'flows',\n",
       " 'Africa',\n",
       " 'increased',\n",
       " 'appreciable',\n",
       " 'way',\n",
       " 'Given',\n",
       " 'sacrifices',\n",
       " 'African',\n",
       " 'countries',\n",
       " 'made',\n",
       " 'keep',\n",
       " 'part',\n",
       " 'bargain',\n",
       " 'important',\n",
       " 'Programme',\n",
       " 'made',\n",
       " 'work',\n",
       " 'Since',\n",
       " 'review',\n",
       " 'Programme',\n",
       " 'take',\n",
       " 'place',\n",
       " '1991',\n",
       " 'programme',\n",
       " 'run',\n",
       " '1990',\n",
       " 'important',\n",
       " 'Programme',\n",
       " 'mandated',\n",
       " 'run',\n",
       " 'final',\n",
       " 'review',\n",
       " 'hope',\n",
       " 'decision',\n",
       " 'taken',\n",
       " 'session',\n",
       " 'Problems',\n",
       " 'environment',\n",
       " 'affect',\n",
       " 'entire',\n",
       " 'globe',\n",
       " 'common',\n",
       " 'abode',\n",
       " 'many',\n",
       " 'speakers',\n",
       " 'referred',\n",
       " 'important',\n",
       " 'factor',\n",
       " 'From',\n",
       " 'depletion',\n",
       " 'ozone',\n",
       " 'layer',\n",
       " 'poisoning',\n",
       " 'atmosphere',\n",
       " 'rivers',\n",
       " 'degradation',\n",
       " 'environment',\n",
       " 'affects',\n",
       " 'us',\n",
       " 'The',\n",
       " 'global',\n",
       " 'warming',\n",
       " 'general',\n",
       " 'climate',\n",
       " 'affects',\n",
       " 'com',\n",
       " 'tries',\n",
       " 'Europe',\n",
       " 'Americas',\n",
       " 'Africa',\n",
       " 'everywhere',\n",
       " 'For',\n",
       " 'us',\n",
       " 'Africa',\n",
       " 'problem',\n",
       " 'environment',\n",
       " 'important',\n",
       " 'face',\n",
       " 'one',\n",
       " 'representative',\n",
       " 'described',\n",
       " 'encroaching',\n",
       " 'deserts',\n",
       " 'We',\n",
       " 'heard',\n",
       " 'many',\n",
       " 'delegations',\n",
       " 'encroachment',\n",
       " 'desert',\n",
       " 'fertile',\n",
       " 'lands',\n",
       " 'We',\n",
       " 'also',\n",
       " 'related',\n",
       " 'problem',\n",
       " 'dumping',\n",
       " 'nuclear',\n",
       " 'toxic',\n",
       " 'wastes',\n",
       " 'subject',\n",
       " 'resolutions',\n",
       " 'Organization',\n",
       " 'African',\n",
       " 'Unity',\n",
       " 'OAU',\n",
       " 'International',\n",
       " 'Atomic',\n",
       " 'Energy',\n",
       " 'Agency',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'last',\n",
       " 'year',\n",
       " 'Greater',\n",
       " 'responsibility',\n",
       " 'needed',\n",
       " 'way',\n",
       " 'world',\n",
       " 'uses',\n",
       " 'natural',\n",
       " 'resources',\n",
       " 'disposes',\n",
       " 'dangerous',\n",
       " 'waste',\n",
       " 'products',\n",
       " 'factories',\n",
       " 'In',\n",
       " 'last',\n",
       " 'year',\n",
       " 'representatives',\n",
       " 'read',\n",
       " 'press',\n",
       " 'reports',\n",
       " 'dangerous',\n",
       " 'dumping',\n",
       " 'wastes',\n",
       " 'certain',\n",
       " 'African',\n",
       " 'countries',\n",
       " 'action',\n",
       " 'rightly',\n",
       " 'condemned',\n",
       " 'OAU',\n",
       " 'internal',\n",
       " 'forums',\n",
       " 'We',\n",
       " 'therefore',\n",
       " 'welcome',\n",
       " 'intention',\n",
       " 'hold',\n",
       " 'special',\n",
       " 'international',\n",
       " 'conference',\n",
       " 'environment',\n",
       " '1992',\n",
       " 'We',\n",
       " 'also',\n",
       " 'aware',\n",
       " 'often',\n",
       " 'weak',\n",
       " 'ate',\n",
       " 'made',\n",
       " 'pay',\n",
       " 'sins',\n",
       " 'strong',\n",
       " 'controlling',\n",
       " 'strong',\n",
       " 'much',\n",
       " 'difficult',\n",
       " 'It',\n",
       " 'important',\n",
       " 'burden',\n",
       " 'attendant',\n",
       " 'upon',\n",
       " 'safeguarding',\n",
       " 'environment',\n",
       " 'shared',\n",
       " 'judiciously',\n",
       " 'according',\n",
       " 'resource',\n",
       " 'usage',\n",
       " 'waste',\n",
       " 'production',\n",
       " 'need',\n",
       " 'Particularly',\n",
       " 'essential',\n",
       " 'new',\n",
       " 'obstacles',\n",
       " 'put',\n",
       " 'path',\n",
       " 'economic',\n",
       " 'progress',\n",
       " 'developing',\n",
       " 'countries',\n",
       " 'already',\n",
       " 'receiving',\n",
       " 'end',\n",
       " 'imbalances',\n",
       " 'inequities',\n",
       " 'present',\n",
       " 'international',\n",
       " 'economic',\n",
       " 'order',\n",
       " 'abuse',\n",
       " 'misuse',\n",
       " 'international',\n",
       " 'environment',\n",
       " 'Respect',\n",
       " 'right',\n",
       " 'peoples',\n",
       " 'selfdetermination',\n",
       " 'independence',\n",
       " 'also',\n",
       " 'good',\n",
       " 'politics',\n",
       " 'essential',\n",
       " 'attainment',\n",
       " 'maintenance',\n",
       " 'world',\n",
       " 'peace',\n",
       " 'The',\n",
       " 'General',\n",
       " 'Assembly',\n",
       " 'clearly',\n",
       " 'recognized',\n",
       " 'fact',\n",
       " 'adopted',\n",
       " 'Declaration',\n",
       " 'Granting',\n",
       " 'Independence',\n",
       " 'Colonial',\n",
       " 'Countries',\n",
       " 'Peoples',\n",
       " 'way',\n",
       " 'back',\n",
       " 'December',\n",
       " '1960',\n",
       " 'Yet',\n",
       " 'today',\n",
       " 'nearly',\n",
       " 'three',\n",
       " 'decades',\n",
       " 'later',\n",
       " 'colonialism',\n",
       " 'still',\n",
       " 'much',\n",
       " 'evidence',\n",
       " 'Again',\n",
       " 'practical',\n",
       " 'reality',\n",
       " 'times',\n",
       " 'The',\n",
       " 'right',\n",
       " 'peoples',\n",
       " 'decide',\n",
       " 'destiny',\n",
       " 'denied',\n",
       " 'wellpublicized',\n",
       " 'cases',\n",
       " 'southern',\n",
       " 'Africa',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'especially',\n",
       " 'Palestine',\n",
       " 'also',\n",
       " 'countries',\n",
       " 'Pacific',\n",
       " 'Caribbean',\n",
       " 'basin',\n",
       " 'elsewhere',\n",
       " 'However',\n",
       " 'major',\n",
       " 'issue',\n",
       " 'facing',\n",
       " 'Organization',\n",
       " 'year',\n",
       " 'decolonization',\n",
       " 'Namibia',\n",
       " 'That',\n",
       " 'The',\n",
       " 'birth',\n",
       " 'new',\n",
       " 'nation',\n",
       " 'supreme',\n",
       " 'event',\n",
       " 'international',\n",
       " 'affairs',\n",
       " 'And',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'appointed',\n",
       " 'intermediary',\n",
       " 'event',\n",
       " 'becomes',\n",
       " 'even',\n",
       " 'special',\n",
       " 'us',\n",
       " 'We',\n",
       " 'ate',\n",
       " 'entering',\n",
       " 'horns',\n",
       " 'stretch',\n",
       " 'implementation',\n",
       " 'Namibian',\n",
       " 'independence',\n",
       " 'plan',\n",
       " 'Much',\n",
       " 'ground',\n",
       " 'already',\n",
       " 'covered',\n",
       " 'much',\n",
       " 'reference',\n",
       " 'made',\n",
       " 'phenomenon',\n",
       " 'speeches',\n",
       " 'heard',\n",
       " 'last',\n",
       " 'two',\n",
       " 'weeks',\n",
       " 'But',\n",
       " 'lot',\n",
       " 'still',\n",
       " 'remains',\n",
       " 'done',\n",
       " 'remaining',\n",
       " 'fourandahalf',\n",
       " 'weeks',\n",
       " 'Pretoria',\n",
       " 'functionaries',\n",
       " 'Namibia',\n",
       " 'still',\n",
       " 'refusing',\n",
       " 'fully',\n",
       " 'comply',\n",
       " 'Security',\n",
       " 'Council',\n",
       " 'resolution',\n",
       " '435',\n",
       " '1978',\n",
       " 'South',\n",
       " 'West',\n",
       " 'Africa',\n",
       " 'Territory',\n",
       " 'SWATP',\n",
       " 'command',\n",
       " 'structure',\n",
       " 'disbanded',\n",
       " 'end',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "tokens_reduced = rem_punc_stop(speech)\n",
    "tokens_reduced\n",
    "#type(tokens_reduced)\n",
    "#fd_reduced = nltk.collocations.FreqDist(tokens_reduced)\n",
    "#fd_reduced.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our analysis is much more informational and revealing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>POS tagging</b> The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 'IN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('outset', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('express', 'VB')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(tokens[2:8])\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming <a id='section 2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP it is often the case that the specific form of a word is not as important as the idea to which it refers. For example, if you are trying to identify the topic of a document, counting 'running', 'runs', 'ran', and 'run' as four separate words is not useful. Reducing words to their stems is a process called stemming.\n",
    "\n",
    "A popular stemming implementation is the Snowball Stemmer, which is based on the Porter Stemmer. Its algorithm looks at word forms and does things like drop final 's's, 'ed's, and 'ing's.\n",
    "\n",
    "Just like the tokenizers, we first have to create a stemmer object with the language we are using. Refer to [this documentation](http://www.nltk.org/howto/stem.html) to create a snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = nltk.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try stemming some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embarass'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('embarassed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball is a very fast algorithm, but it has a lot of edge cases. In some cases, words with the same stem are reduced to two different stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cylind', 'cylindr')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('cylinder'), snowball.stem('cylindrical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes two different words are reduced to the same stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vacat', 'vacat')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('vacation'), snowball.stem('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b> How would the above two situations affect our text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking<a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to work with larger segments of text than single words (but still smaller than a sentence). For instance, in the sentence \"The black cat climbed over the tall fence\", we might want to treat \"The black cat\" as one thing (the subject), \"climbed over\" as a distinct act, and \"the tall fence\" as another thing (the object). The first and third sequences are noun phrases, and the second is a verb phrase.\n",
    "\n",
    "We can separate these phrases by \"chunking\" the sentence, i.e. splitting it into larger chunks than individual tokens. This is also an important step toward identifying entities, which are often represented by more than one word. You can probably imagine certain patterns that would define a noun phrase, using part of speech tags. For instance, a determiner (e.g. an article like \"the\") could be concatenated onto the noun that follows it. If there's an adjective between them, we can include that too.\n",
    "\n",
    "To define rules about how to structure words based on their part of speech tags, we use a grammar (in this case, a \"chunk grammar\"). NLTK provides a RegexpParser that takes as input a grammar composed of regular expressions (which define patterns in text, we'll learn it in later labs). The grammar is defined as a string, with one line for each rule we define. Each rule starts with the label we want to assign to the chunk (e.g. NP for \"noun phrase\"), followed by a colon, then an expression in regex-like notation that will be matched to tokens' POS (part-of-speech) tags.\n",
    "\n",
    "We can define a single rule for a noun phrase like this. The rule allows 0 or 1 determiner, then 0 or more adjectives, and finally at least 1 noun. (By using 'NN.*' as the last POS tag, we can match 'NN', 'NNP' for a proper noun, or 'NNS' for a plural noun.) If a matching sequence of tokens is found, it will be labeled 'NP'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at different [POS tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a chunk parser object by supplying this grammar, then use it to parse a sentence into chunks. The sentence we want to parse must already be POS-tagged, since our grammar uses those POS tags to identify chunks. Let's try this on the second sentence of the speech we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP T/NNP h/NN e/NN  /NNP)\n",
      "  u/JJ\n",
      "  n/FW\n",
      "  (NP a/DT n/NN i/NN)\n",
      "  m/VBP\n",
      "  (NP i/NN)\n",
      "  t/VBP\n",
      "  (NP y/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP f/JJ  /NNP t/NN)\n",
      "  h/VBD\n",
      "  (NP a/DT t/NN  /NNP d/NN e/NN c/NN i/NN)\n",
      "  s/VBP\n",
      "  (NP i/NN)\n",
      "  o/VBP\n",
      "  (NP n/JJ  /NNP r/NN e/NN f/NN l/NN e/NN)\n",
      "  c/VBP\n",
      "  (NP t/NN s/NN  /NNP)\n",
      "  n/CC\n",
      "  (NP o/JJ t/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP n/JJ l/NN y/NN  /NNP y/NNP)\n",
      "  o/MD\n",
      "  u/VB\n",
      "  (NP r/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP w/JJ n/JJ  /NNP d/NN i/NN)\n",
      "  s/VBP\n",
      "  (NP t/NN i/NN)\n",
      "  n/VBP\n",
      "  (NP g/NN u/NN i/NN)\n",
      "  s/VBP\n",
      "  (NP h/NN e/NN d/NN  /NNP r/NN e/NN c/NN o/NN r/NN d/NN)\n",
      "   /VBZ\n",
      "  (NP a/DT s/JJ  /NN F/NNP)\n",
      "  o/VBZ\n",
      "  (NP r/NN e/NN i/NN)\n",
      "  g/VBP\n",
      "  (NP n/JJ  /NNP M/NNP i/NN)\n",
      "  n/VBP\n",
      "  (NP i/NN)\n",
      "  s/VBP\n",
      "  (NP t/NN e/NN r/NN)\n",
      "   /VBZ\n",
      "  (NP a/DT n/JJ d/NN  /NNP P/NNP e/NN r/NN)\n",
      "  m/VBD\n",
      "  (NP a/DT n/JJ e/NN)\n",
      "  (NP n/JJ t/NN  /NNP R/NNP e/NN p/NN r/NN e/NN s/NN e/NN)\n",
      "  n/JJ\n",
      "  t/VBZ\n",
      "  (NP a/DT t/NN i/NN)\n",
      "  v/VBP\n",
      "  (NP e/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP f/JJ  /NNP y/NN o/NN)\n",
      "  (NP u/JJ r/NN  /NNP)\n",
      "  c/VBZ\n",
      "  (NP\n",
      "    o/JJ\n",
      "    u/JJ\n",
      "    n/NN\n",
      "    t/NN\n",
      "    r/NN\n",
      "    y/NN\n",
      "     /NNP\n",
      "    t/NN\n",
      "    o/NN\n",
      "     /NNP\n",
      "    t/NN\n",
      "    h/NN\n",
      "    e/NN\n",
      "     /NNP\n",
      "    U/NNP\n",
      "    n/NN\n",
      "    i/NN)\n",
      "  t/VBP\n",
      "  (NP e/NN d/NN  /NNP N/NNP)\n",
      "  (NP a/DT t/NN i/NN)\n",
      "  o/VBP\n",
      "  (NP n/JJ s/NN  /NN b/NN)\n",
      "  (NP u/JJ t/NN)\n",
      "   /VBD\n",
      "  (NP a/DT l/NN s/NN o/NN  /NNP t/NN h/NN e/NN  /NNP p/NN r/NN e/NN)\n",
      "  s/VBD\n",
      "  (NP t/NN i/NN)\n",
      "  g/VBP\n",
      "  (NP e/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP f/JJ  /NNP y/NN o/NN)\n",
      "  (NP u/JJ r/NN  /NNP)\n",
      "  c/VBZ\n",
      "  (NP o/JJ u/JJ n/NN t/NN r/NN y/NN)\n",
      "  ,/,\n",
      "  (NP  /NNP N/NNP i/NN)\n",
      "  g/VBP\n",
      "  (NP e/NN r/NN)\n",
      "  i/VBZ\n",
      "  a/DT\n",
      "  ,/,\n",
      "  (NP  /JJ o/NN f/NN  /NNP w/NN h/NN)\n",
      "  i/JJ\n",
      "  c/VBP\n",
      "  h/JJ\n",
      "   /FW\n",
      "  (NP a/DT l/NN l/NN  /NNP)\n",
      "  o/VBZ\n",
      "  (NP f/JJ  /NNP u/NN s/NN  /NN i/NN)\n",
      "  n/VBP\n",
      "   /PDT\n",
      "  (NP A/NNP f/NN r/NN i/NN)\n",
      "  c/VBP\n",
      "  (NP a/DT  /NN)\n",
      "  (NP a/DT r/NN e/NN  /NNP p/NN r/NN o/NN)\n",
      "  (NP u/JJ d/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "\n",
    "sent_tagged = nltk.pos_tag(sents[1])\n",
    "sent_chunked = cp.parse(sent_tagged)\n",
    "\n",
    "print(sent_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we called print() on this chunked sentence, it printed out a nested list of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree object has a number of methods we can use to interact with its components. For instance, we can use the method draw() to see a more graphical representation. This will open a separate window.\n",
    "\n",
    "The tree is pretty flat, because we defined a grammar that only grouped words into non-overlapping noun phrases, with no additional hierarchy above them. This is sometimes referred to as \"shallow parsing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in a strubg, tokenizes it, removes noise, turns everything to lower-case, and returns a string of stems of all tokens.\n",
    "\n",
    "Hint: any function from above that we can just grab and use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, this is what our table looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def does_it_all(text):\n",
    "    tokens = \"\"\n",
    "    \n",
    "    not_stemmed = rem_punc_stop(text)\n",
    "    stemmed = [snowball.stem(word.lower()) for word in not_stemmed]\n",
    "    for word in not_stemmed:\n",
    "        tokens += snowball.stem(word) + \" \"\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our function to speeches from 2001.\n",
    "\n",
    "First create a table that includes all 2001 speeches. Refer to [this doc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_2001 = data.loc[data['year'] == 2001]\n",
    "speech_2001.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a new column in speech_2001 which contains the tokenized string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_2001_with_tokens = speech_2001.copy()\n",
    "speech_2001_with_tokens['tokens'] = speech_2001['text'].apply(does_it_all)\n",
    "speech_2001_with_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've learned tokenizing, stemming, and chunking texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Tian Qin\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
